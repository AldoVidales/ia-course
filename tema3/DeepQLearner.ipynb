{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1SHs6riAj1s",
        "colab_type": "text"
      },
      "source": [
        "# Clonamos el repositorio para obtener los dataSet\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WFfFZvPApeD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "aedf8600-36ba-4fe4-d83c-c98422a4ed42"
      },
      "source": [
        "!git clone https://github.com/joanby/ia-course.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ia-course'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 380 (delta 0), reused 2 (delta 0), pack-reused 375\u001b[K\n",
            "Receiving objects: 100% (380/380), 32.57 MiB | 9.30 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZZLJUFyA6hs",
        "colab_type": "text"
      },
      "source": [
        "# Damos acceso a nuestro Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMVaYKVrA92v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8CYKTxkJB53",
        "colab_type": "text"
      },
      "source": [
        "# Test it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COqkUYzLJBEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls '/content/drive/My Drive' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eas_Lb17J4jY",
        "colab_type": "text"
      },
      "source": [
        "#Google colab tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Kmgf5ZJ5QV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "import glob # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "from google.colab import drive # Montar tu Google drive"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "# Instalar dependencias de Renderizado, tarda alrededor de 45 segundos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDPDMlDo4RqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install python-opengl -y > /dev/null 2>&1\n",
        "!apt install xvfb -y --fix-missing > /dev/null 2>&1\n",
        "!apt-get install ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!apt-get install pyglet > /dev/null 2>&1\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xxsEvp14U-Q",
        "colab_type": "text"
      },
      "source": [
        "# Instalar OpenAi Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCuDUcBL4SCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install piglet > /dev/null 2>&1\n",
        "!pip install 'gym[box2d]' > /dev/null 2>&1\n",
        "#por si quieres algun environment en concreto\n",
        "#!pip install atari_py > /dev/null 2>&1\n",
        "#!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkkUdIpJ5lHQ",
        "colab_type": "text"
      },
      "source": [
        "# Todos los imports necesarios en google colab y helpers para poder visualizar OpenAi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbgBYNVD5i6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5DvRsCZ4b-G",
        "colab_type": "text"
      },
      "source": [
        "# Activamos una vista, seria como crear un plot de una grafica en python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDMnJvk-4cec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8d18350-58bb-4c1c-81bb-db19f0216928"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900)) #Puedes modificar el high and width de la pantalla\n",
        "display.start()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fc006eba240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x80iaLF94eOh",
        "colab_type": "text"
      },
      "source": [
        "# Este código crea una pantalla virtual para dibujar imágenes del juego. \n",
        "## Si se ejecuta localmente, simplemente ignóralo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0H-whKQ4fvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "if type(os.environ.get('DISPLAY')) is not str or \\\n",
        "        len(os.environ.get('DISPLAY')) == 0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMqg9TAA4qxy",
        "colab_type": "text"
      },
      "source": [
        "# Funciones de utilidad para permitir la grabación de video del ambiente del gimnasio y su visualización\n",
        "## Para habilitar la visualizacion por pantalla , tan solo haz \"**environment = wrap_env(environment)**\", por ejemplo: **environment = wrap_env(gym.make(\"MountainCar-v0\"))**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKFIMV_l4m2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import glob\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pathlib import Path\n",
        "\n",
        "def show_one_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "\n",
        "        content = ipythondisplay.display(HTML(data='''\n",
        "        <video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "        </video>\n",
        "        '''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Couldn't find video\")\n",
        "\n",
        "def show_videos(video_path='video', prefix=''):\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = gym.wrappers.Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpaK1CCiVVBR",
        "colab_type": "text"
      },
      "source": [
        "# Instalar pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udH-oiduVdI6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "cdcb18c0-273f-48ae-8083-64e9086deeff"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install numpy\n",
        "!pip install tensorboardX"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.6.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeCCtgxDBiGi",
        "colab_type": "text"
      },
      "source": [
        "# Nuestro Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBaH0bgAv1KR",
        "colab_type": "text"
      },
      "source": [
        "En google colab, para importar modulos se hace de manera diferente, ya que los notebooks que crea se guardan en una ruta temporal para despues guardarse en drive, por lo que si quieres cargar modulos externos se tiene que hacer:  \n",
        "**import sys**   \n",
        "**sys.path.append('path-carpeta')**  *#esto añade como una caché de rutas*  \n",
        "*from module import function or class*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMipgf50VLnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/ia-course/tema3/libs/')\n",
        "sys.path.append('/content/ia-course/tema3/utils/')\n",
        "sys.path.append('/content/ia-course/tema3/environments/')\n",
        "\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from datetime import datetime\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from perceptron import SLP\n",
        "from cnn import CNN\n",
        "\n",
        "from decay_schedule import LinearDecaySchedule\n",
        "from experience_memory import ExperienceMemory, Experience\n",
        "from params_manager import ParamsManager\n",
        "\n",
        "import atari as Atari\n",
        "import utils as env_utils\n",
        "\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75d5N1gLJnED",
        "colab_type": "text"
      },
      "source": [
        "# Parseador de Argumentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tptnugixJpzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = ArgumentParser(\"DeepQLearning\")\n",
        "args.add_argument(\"--params-file\", help = \"Path del fichero JSON de parámetros. El valor por defecto es parameters.json\",\n",
        "                  default=\"/content/ia-course/tema3/parameters.json\", metavar = \"PFILE\")\n",
        "args.add_argument(\"--env\", help = \"Entorno de ID de Atari disponible en OpenAI Gym. El valor por defecto será SeaquestNoFrameskip-v4\",\n",
        "                  default = \"SeaquestNoFrameskip-v4\", metavar=\"ENV\")\n",
        "args.add_argument(\"--gpu-id\", help = \"ID de la GPU a utilizar, por defecto 0\", default = 0, type = int, metavar = \"GPU_ID\")\n",
        "args.add_argument(\"--test\", help = \"Modo de testing para jugar sin aprender. Por defecto está desactivado\", \n",
        "                  action = \"store_true\", default = False)\n",
        "args.add_argument(\"--render\", help = \"Renderiza el entorno en pantalla. Desactivado por defecto\", action=\"store_true\", default=False)\n",
        "args.add_argument(\"--record\", help = \"Almacena videos y estados de la performance del agente\", action=\"store_true\", default=False)\n",
        "args.add_argument(\"--output-dir\", help = \"Directorio para almacenar los outputs. Por defecto = ./trained_models/results\",\n",
        "                  default = \"./trained_models/results\")\n",
        "args, _ = args.parse_known_args()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MewlOwwLJ1Ot",
        "colab_type": "text"
      },
      "source": [
        "# Parámetros globales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOKy9uUWJ2WX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "manager = ParamsManager('/content/ia-course/tema3/parameters.json')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcW__8l6J7pA",
        "colab_type": "text"
      },
      "source": [
        "# Ficheros de logs acerca de la configuración de las ejecuciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qsMEjXSJ82U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_filename_prefix = manager.get_agent_params()['summary_filename_prefix']\n",
        "summary_filename = summary_filename_prefix + args.env + datetime.now().strftime(\"%y-%m-%d-%H-%M\")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0b4a6a_FzMO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Summary Writer de TensorBoardX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MIWMd8GF1tL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer = SummaryWriter(summary_filename)\n",
        "\n",
        "manager.export_agent_params(summary_filename + \"/\"+\"agent_params.json\")\n",
        "manager.export_environment_params(summary_filename + \"/\"+\"environment_params.json\")"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VnM12lYF6Tk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#Contador global de ejecuciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVbbs8v7F8JR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step_num = 0"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KshEAhVWF_YJ",
        "colab_type": "text"
      },
      "source": [
        "# Habilitar entrenamiento por gráfica o CPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEbloYS7F-tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = manager.get_agent_params()['use_cuda']\n",
        "device = torch.device(\"cuda:\"+str(args.gpu_id) if torch.cuda.is_available() and use_cuda else \"cpu\")"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyhWLSs4GEBL",
        "colab_type": "text"
      },
      "source": [
        "# Habilitar la semilla aleatoria para poder reproducir el experimento a posteriori"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPG2SQKIEhs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = manager.get_agent_params()['seed']\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "if torch.cuda.is_available() and use_cuda:\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnBymhWkEau6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepQLearner(object):\n",
        "    def __init__(self, obs_shape, action_shape, params):\n",
        "       \n",
        "        self.params = params\n",
        "        self.gamma = self.params['gamma']\n",
        "        self.learning_rate = self.params['learning_rate']\n",
        "        self.best_mean_reward = -float(\"inf\")\n",
        "        self.best_reward = -float(\"inf\")\n",
        "        self.training_steps_completed = 0\n",
        "        self.action_shape = action_shape\n",
        "        \n",
        "        if len(obs_shape)  == 1: ## Solo tenemos una dimensión del espacio de observaciones\n",
        "            self.DQN = SLP\n",
        "        elif len(obs_shape) == 3: ## El estado de observaciones es una imagen/3D\n",
        "            self.DQN = CNN\n",
        "            \n",
        "        self.Q = self.DQN(obs_shape, action_shape, device).to(device)\n",
        "        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr = self.learning_rate)\n",
        "        \n",
        "        if self.params['use_target_network']:\n",
        "            self.Q_target = self.DQN(obs_shape, action_shape, device).to(device)\n",
        "           \n",
        "        self.policy = self.epsilon_greedy_Q\n",
        "        self.epsilon_max = self.params['epsilon_max']\n",
        "        self.epsilon_min = self.params['epsilon_min']\n",
        "        self.epsilon_decay = LinearDecaySchedule(initial_value = self.epsilon_max,\n",
        "                                                 final_value = self.epsilon_min, \n",
        "                                                 max_steps = self.params['epsilon_decay_final_step'])\n",
        "        self.step_num = 0\n",
        "        \n",
        "        self.memory = ExperienceMemory(capacity = int(self.params['experience_memory_size']))\n",
        "        \n",
        "         \n",
        "    def get_action(self, obs):\n",
        "        obs = np.array(obs)\n",
        "        obs = obs / 255.0\n",
        "        if len(obs.shape) == 3: # tenemos una imagen\n",
        "            if obs.shape[2] < obs.shape[0]: # WxHxC -> C x H x W\n",
        "                obs = obs.reshape(obs.shape[2], obs.shape[1], obs.shape[0])\n",
        "            obs = np.expand_dims(obs, 0)   \n",
        "        return self.policy(obs)\n",
        "    \n",
        "    def epsilon_greedy_Q(self, obs):\n",
        "        writer.add_scalar(\"DQL/epsilon\", self.epsilon_decay(self.step_num), self.step_num)\n",
        "        self.step_num +=1\n",
        "        if random.random() < self.epsilon_decay(self.step_num) and not self.params[\"test\"]:\n",
        "            action = random.choice([a for a in range(self.action_shape)])\n",
        "        else:\n",
        "            action = np.argmax(self.Q(obs).data.to(torch.device('cpu')).numpy())   \n",
        "        return action\n",
        "        \n",
        "        \n",
        "    def learn(self, obs, action, reward, next_obs, done):\n",
        "        if done:\n",
        "            td_target = reward + 0.0\n",
        "        else: \n",
        "            td_target = reward + self.gamma * torch.max(self.Q(next_obs))\n",
        "        td_error = torch.nn.functional.mse_loss(self.Q(obs)[action], td_target)\n",
        "        self.Q_optimizer.zero_grad()\n",
        "        td_error.backward()\n",
        "        writer.add_scalar(\"DQL/td_error\", td_error.mean(), self.step_num)\n",
        "        self.Q_optimizer.step()\n",
        "        \n",
        "    def replay_experience(self, batch_size = None):\n",
        "        \"\"\"\n",
        "        Vuelve a jugar usando la experiencia aleatoria almacenada\n",
        "        :param batch_size: Tamaño de la muestra a tomar de la memoria\n",
        "        :return: \n",
        "        \"\"\"\n",
        "        batch_size = batch_size if batch_size is not None else self.params['replay_batch_size']\n",
        "        experience_batch = self.memory.sample(batch_size)\n",
        "        self.learn_from_batch_experience(experience_batch)   \n",
        "        self.training_steps_completed += 1\n",
        "      \n",
        "    def learn_from_batch_experience(self, experiences):\n",
        "        \"\"\"\n",
        "        Actualiza la red neuronal profunda en base a lo aprendido en el conjunto de experiencias anteriores\n",
        "        :param experiences: fragmento de recuerdos anteriores\n",
        "        :return: \n",
        "        \"\"\"\n",
        "        batch_xp = Experience(*zip(*experiences))\n",
        "        obs_batch = np.array(batch_xp.obs)/255.0\n",
        "        action_batch = np.array(batch_xp.action)\n",
        "        reward_batch = np.array(batch_xp.reward)\n",
        "        \n",
        "        if self.params[\"clip_reward\"]:\n",
        "            reward_batch = np.sign(reward_batch)\n",
        "        next_obs_batch = np.array(batch_xp.next_obs)/255.0\n",
        "        done_batch = np.array(batch_xp.done)\n",
        "        \n",
        "        \n",
        "        if self.params['use_target_network']:\n",
        "            if self.step_num % self.params['target_network_update_frequency'] == 0:\n",
        "                self.Q_target.load_state_dict(self.Q.state_dict())\n",
        "            td_target = reward_batch + ~done_batch *\\\n",
        "                        np.tile(self.gamma, len(next_obs_batch)) * \\\n",
        "                        self.Q_target(next_obs_batch).max(1)[0].data\n",
        "        else: \n",
        "            td_target = reward_batch + ~done_batch * \\\n",
        "                        np.tile(self.gamma, len(next_obs_batch)) * \\\n",
        "                        self.Q(next_obs_batch).detach().max(1)[0].data\n",
        "        \n",
        "        td_target = td_target.to(device)\n",
        "        action_idx = torch.from_numpy(action_batch).to(device)\n",
        "        td_error = torch.nn.functional.mse_loss(\n",
        "                self.Q(obs_batch).gather(1, action_idx.view(-1,1)),\n",
        "                td_target.float().unsqueeze(1))\n",
        "        \n",
        "        self.Q_optimizer.zero_grad()\n",
        "        td_error.mean().backward()\n",
        "        self.Q_optimizer.step()\n",
        "        \n",
        "    def save(self, env_name):\n",
        "        file_name = self.params['save_dir']+\"DQL_\"+env_name+\".ptm\"\n",
        "        agent_state = {\"Q\": self.Q.state_dict(),\n",
        "                       \"best_mean_reward\": self.best_mean_reward,\n",
        "                       \"best_reward\": self.best_reward}\n",
        "        torch.save(agent_state, file_name)\n",
        "        print(\"Estado del agente guardado en : \", file_name)\n",
        "        \n",
        "        \n",
        "    def load(self, env_name):\n",
        "        file_name = self.params['load_dir']+\"DQL_\"+env_name+\".ptm\"\n",
        "        agent_state = torch.load(file_name, map_location = lambda storage, loc: storage)\n",
        "        self.Q.load_state_dict(agent_state[\"Q\"])\n",
        "        self.Q.to(device)\n",
        "        self.best_mean_reward = agent_state[\"best_mean_reward\"]\n",
        "        self.best_reward = agent_state[\"best_reward\"]\n",
        "        print(\"Cargado del modelo Q desde\", file_name,\n",
        "              \"que hasta el momento tiene una mejor recompensa media de: \",self.best_mean_reward,\n",
        "              \" y una recompensa máxima de: \", self.best_reward)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg5F6OERzRvD",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d39qUlVjENDH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "608a2ae0-980d-4a6d-b009-b5642b19915b"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env_conf = manager.get_environment_params()\n",
        "    env_conf[\"env_name\"] = args.env\n",
        "    \n",
        "    if args.test:\n",
        "        env_conf[\"episodic_life\"] = False\n",
        "    reward_type = \"LIFE\" if env_conf[\"episodic_life\"] else \"GAME\"\n",
        "    \n",
        "    custom_region_available = False\n",
        "    for key, value in env_conf[\"useful_region\"].items():\n",
        "        if key in args.env:\n",
        "            env_conf[\"useful_region\"] = value\n",
        "            custom_region_available = True\n",
        "            break\n",
        "    if custom_region_available is not True:\n",
        "        env_conf[\"useful_region\"] = env_conf[\"useful_region\"][\"Default\"]\n",
        "    print(\"Configuración a utilizar:\", env_conf)\n",
        "    \n",
        "    atari_env = False\n",
        "    for game in Atari.get_games_list():\n",
        "        if game.replace(\"_\", \"\") in args.env.lower():\n",
        "            atari_env = True\n",
        "    \n",
        "    if atari_env:\n",
        "        environment = Atari.make_env(args.env, env_conf)\n",
        "    else:\n",
        "        environment = env_utils.ResizeReshapeFrames(gym.make(args.env))\n",
        "        \n",
        "    obs_shape = environment.observation_space.shape\n",
        "    action_shape = environment.action_space.n\n",
        "    agent_params = manager.get_agent_params()\n",
        "    agent_params[\"test\"] = args.test\n",
        "    agent_params[\"clip_reward\"] = env_conf[\"clip_reward\"]\n",
        "    agent = DeepQLearner(obs_shape, action_shape, agent_params)\n",
        "    \n",
        "    episode_rewards = list()\n",
        "    previous_checkpoint_mean_ep_rew = agent.best_mean_reward\n",
        "    num_improved_episodes_before_checkpoint = 0\n",
        "    if agent_params['load_trained_model']:\n",
        "        try:\n",
        "            agent.load(env_conf['env_name'])\n",
        "            previous_checkpoint_mean_ep_rew = agent.best_mean_reward\n",
        "        except FileNotFoundError:\n",
        "            print(\"ERROR: no existe ningún modelo entrenado para este entorno. Empezamos desde cero\")\n",
        "\n",
        "    \n",
        "    episode = 0\n",
        "    while global_step_num < agent_params['max_training_steps']:\n",
        "        obs = environment.reset()\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        step = 0\n",
        "        while not done: \n",
        "            if env_conf['render'] or args.render:\n",
        "                environment.render()\n",
        "            \n",
        "            action = agent.get_action(obs)\n",
        "            next_obs, reward, done, info = environment.step(action)\n",
        "            agent.memory.store(Experience(obs, action, reward, next_obs, done))\n",
        "            \n",
        "            obs = next_obs\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "            global_step_num += 1\n",
        "            \n",
        "            if done is True:\n",
        "                episode += 1\n",
        "                episode_rewards.append(total_reward)\n",
        "            \n",
        "                if total_reward > agent.best_reward:\n",
        "                    agent.best_reward = total_reward\n",
        "                \n",
        "                if np.mean(episode_rewards) > previous_checkpoint_mean_ep_rew: \n",
        "                    num_improved_episodes_before_checkpoint += 1\n",
        "                \n",
        "                if num_improved_episodes_before_checkpoint >= agent_params['save_freq']:\n",
        "                    previous_checkpoint_mean_ep_rew = np.mean(episode_rewards)\n",
        "                    agent.best_mean_reward = np.mean(episode_rewards)\n",
        "                    agent.save(env_conf['env_name'])\n",
        "                    num_improved_episodes_before_checkpoint = 0\n",
        "                \n",
        "                print(\"\\n Episodio #{} finalizado con {} iteraciones. Con {} estados: recompensa = {}, recompensa media = {:.2f}, mejor recompensa = {}\".\n",
        "                      format(episode, step+1, reward_type, total_reward, np.mean(episode_rewards), agent.best_reward))\n",
        "                \n",
        "                writer.add_scalar(\"main/ep_reward\", total_reward, global_step_num)\n",
        "                writer.add_scalar(\"main/mean_ep_reward\", np.mean(episode_rewards), global_step_num)\n",
        "                writer.add_scalar(\"main/max_ep_reward\", agent.best_reward, global_step_num)\n",
        "                \n",
        "                if agent.memory.get_size() >= 2*agent_params['replay_start_size'] and not args.test:\n",
        "                    agent.replay_experience()\n",
        "                    \n",
        "                break\n",
        "            \n",
        "    environment.close()\n",
        "    writer.close()\n",
        "    show_videos()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Configuración a utilizar: {'type': 'Atari', 'episodic_life': True, 'clip_reward': True, 'skip_rate': 4, 'num_frames_to_stack': 4, 'render': False, 'normalize_observation': False, 'useful_region': {'crop1': 30, 'crop2': 30, 'dimension2': 80}, 'env_name': 'SeaquestNoFrameskip-v4'}\n",
            "ERROR: no existe ningún modelo entrenado para este entorno. Empezamos desde cero\n",
            "\n",
            " Episodio #1 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.00, mejor recompensa = 0.0\n",
            "\n",
            " Episodio #2 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.00, mejor recompensa = 0.0\n",
            "\n",
            " Episodio #3 finalizado con 125 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.33, mejor recompensa = 1.0\n",
            "\n",
            " Episodio #4 finalizado con 145 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.75, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #5 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.60, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #6 finalizado con 118 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.50, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #7 finalizado con 135 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.71, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #8 finalizado con 156 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.75, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #9 finalizado con 109 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.78, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #10 finalizado con 114 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.70, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #11 finalizado con 128 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.73, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #12 finalizado con 134 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.83, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #13 finalizado con 117 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.77, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #14 finalizado con 130 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.86, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #15 finalizado con 123 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.87, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #16 finalizado con 119 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.94, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #17 finalizado con 83 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.88, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #18 finalizado con 62 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.83, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #19 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.79, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #20 finalizado con 85 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.75, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #21 finalizado con 149 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.81, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #22 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.77, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #23 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.74, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #24 finalizado con 91 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.71, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #25 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.68, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #26 finalizado con 218 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.73, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #27 finalizado con 83 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.70, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #28 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.68, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #29 finalizado con 124 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.69, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #30 finalizado con 134 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.70, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #31 finalizado con 64 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.68, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #32 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.66, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #33 finalizado con 143 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.67, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #34 finalizado con 155 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.71, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #35 finalizado con 163 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.71, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #36 finalizado con 118 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.69, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #37 finalizado con 68 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.68, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #38 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.66, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #39 finalizado con 104 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.64, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #40 finalizado con 72 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.62, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #41 finalizado con 128 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.66, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #42 finalizado con 178 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.69, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #43 finalizado con 112 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.70, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #44 finalizado con 136 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.70, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #45 finalizado con 59 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.69, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #46 finalizado con 61 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.67, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #47 finalizado con 120 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.68, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #48 finalizado con 60 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.67, mejor recompensa = 2.0\n",
            "\n",
            " Episodio #49 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.65, mejor recompensa = 2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-63cddffdff22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0mprevious_checkpoint_mean_ep_rew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_mean_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_conf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'env_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                     \u001b[0mnum_improved_episodes_before_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-077b61be6533>\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, env_name)\u001b[0m\n\u001b[1;32m    112\u001b[0m                        \u001b[0;34m\"best_mean_reward\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_mean_reward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                        \"best_reward\": self.best_reward}\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Estado del agente guardado en : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained_models/DQL_SeaquestNoFrameskip-v4.ptm'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lExILnX9VfIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}