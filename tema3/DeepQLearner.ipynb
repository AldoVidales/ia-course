{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1SHs6riAj1s",
        "colab_type": "text"
      },
      "source": [
        "# Clonamos el repositorio para obtener los dataSet\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WFfFZvPApeD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d654f2a-3710-4945-99bd-a4281a047969"
      },
      "source": [
        "!git clone https://github.com/joanby/ia-course.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ia-course' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZZLJUFyA6hs",
        "colab_type": "text"
      },
      "source": [
        "# Damos acceso a nuestro Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMVaYKVrA92v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4d4ad11-7faa-40a2-fe93-bed78bd8a68f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8CYKTxkJB53",
        "colab_type": "text"
      },
      "source": [
        "# Test it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COqkUYzLJBEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls '/content/drive/My Drive' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eas_Lb17J4jY",
        "colab_type": "text"
      },
      "source": [
        "#Google colab tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Kmgf5ZJ5QV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "import glob # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "from google.colab import drive # Montar tu Google drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "# Instalar dependencias de Renderizado, tarda alrededor de 45 segundos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDPDMlDo4RqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install python-opengl -y > /dev/null 2>&1\n",
        "!apt install xvfb -y --fix-missing > /dev/null 2>&1\n",
        "!apt-get install ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!apt-get install pyglet > /dev/null 2>&1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xxsEvp14U-Q",
        "colab_type": "text"
      },
      "source": [
        "# Instalar OpenAi Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCuDUcBL4SCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install piglet > /dev/null 2>&1\n",
        "!pip install 'gym[box2d]' > /dev/null 2>&1\n",
        "#por si quieres algun environment en concreto\n",
        "#!pip install atari_py > /dev/null 2>&1\n",
        "#!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkkUdIpJ5lHQ",
        "colab_type": "text"
      },
      "source": [
        "# Todos los imports necesarios en google colab y helpers para poder visualizar OpenAi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbgBYNVD5i6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5DvRsCZ4b-G",
        "colab_type": "text"
      },
      "source": [
        "# Activamos una vista, seria como crear un plot de una grafica en python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDMnJvk-4cec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7deb00b-298c-4552-890d-d9f9de5423c2"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900)) #Puedes modificar el high and width de la pantalla\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f7c740b7f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x80iaLF94eOh",
        "colab_type": "text"
      },
      "source": [
        "# Este código crea una pantalla virtual para dibujar imágenes del juego. \n",
        "## Si se ejecuta localmente, simplemente ignóralo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0H-whKQ4fvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "if type(os.environ.get('DISPLAY')) is not str or \\\n",
        "        len(os.environ.get('DISPLAY')) == 0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMqg9TAA4qxy",
        "colab_type": "text"
      },
      "source": [
        "# Funciones de utilidad para permitir la grabación de video del ambiente del gimnasio y su visualización\n",
        "## Para habilitar la visualizacion por pantalla , tan solo haz \"**environment = wrap_env(environment)**\", por ejemplo: **environment = wrap_env(gym.make(\"MountainCar-v0\"))**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKFIMV_l4m2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import glob\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pathlib import Path\n",
        "\n",
        "def show_one_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "\n",
        "        content = ipythondisplay.display(HTML(data='''\n",
        "        <video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "        </video>\n",
        "        '''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Couldn't find video\")\n",
        "\n",
        "def show_videos(video_path='video', prefix=''):\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = gym.wrappers.Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpaK1CCiVVBR",
        "colab_type": "text"
      },
      "source": [
        "# Instalar pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udH-oiduVdI6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "90f53b0e-9991-45a2-b101-874859f98d27"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install numpy\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeCCtgxDBiGi",
        "colab_type": "text"
      },
      "source": [
        "# Nuestro Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBaH0bgAv1KR",
        "colab_type": "text"
      },
      "source": [
        "En google colab, para importar modulos se hace de manera diferente, ya que los notebooks que crea se guardan en una ruta temporal para despues guardarse en drive, por lo que si quieres cargar modulos externos se tiene que hacer:  \n",
        "**import sys**   \n",
        "**sys.path.append('path-carpeta')**  *#esto añade como una caché de rutas*  \n",
        "*from module import function or class*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMipgf50VLnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/ia-course/tema3/libs/')\n",
        "sys.path.append('/content/ia-course/tema3/utils/')\n",
        "sys.path.append('/content/ia-course/tema3/environments/')\n",
        "\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from datetime import datetime\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from perceptron import SLP\n",
        "from cnn import CNN\n",
        "\n",
        "from decay_schedule import LinearDecaySchedule\n",
        "from experience_memory import ExperienceMemory, Experience\n",
        "from params_manager_colab import ParamsManager\n",
        "\n",
        "import atari as Atari\n",
        "import utils as env_utils\n",
        "\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75d5N1gLJnED",
        "colab_type": "text"
      },
      "source": [
        "# Parseador de Argumentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tptnugixJpzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = ArgumentParser(\"DeepQLearning\")\n",
        "args.add_argument(\"--params-file\", help = \"Path del fichero JSON de parámetros. El valor por defecto es parameters.json\",\n",
        "                  default=\"/content/ia-course/tema3/parameters.json\", metavar = \"PFILE\")\n",
        "args.add_argument(\"--env\", help = \"Entorno de ID de Atari disponible en OpenAI Gym. El valor por defecto será SeaquestNoFrameskip-v4\",\n",
        "                  default = \"SeaquestNoFrameskip-v4\", metavar=\"ENV\")\n",
        "args.add_argument(\"--gpu-id\", help = \"ID de la GPU a utilizar, por defecto 0\", default = 0, type = int, metavar = \"GPU_ID\")\n",
        "args.add_argument(\"--test\", help = \"Modo de testing para jugar sin aprender. Por defecto está desactivado\", \n",
        "                  action = \"store_true\", default = False)\n",
        "args.add_argument(\"--render\", help = \"Renderiza el entorno en pantalla. Desactivado por defecto\", action=\"store_true\", default=False)\n",
        "args.add_argument(\"--record\", help = \"Almacena videos y estados de la performance del agente\", action=\"store_true\", default=False)\n",
        "args.add_argument(\"--output-dir\", help = \"Directorio para almacenar los outputs. Por defecto = ./trained_models/results\",\n",
        "                  default = \"./trained_models/results\")\n",
        "args, _ = args.parse_known_args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MewlOwwLJ1Ot",
        "colab_type": "text"
      },
      "source": [
        "# Parámetros globales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOKy9uUWJ2WX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "manager = ParamsManager(args.params_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcW__8l6J7pA",
        "colab_type": "text"
      },
      "source": [
        "# Ficheros de logs acerca de la configuración de las ejecuciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qsMEjXSJ82U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_filename_prefix = manager.get_agent_params()['summary_filename_prefix']\n",
        "summary_filename = summary_filename_prefix + args.env + datetime.now().strftime(\"%y-%m-%d-%H-%M\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0b4a6a_FzMO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Summary Writer de TensorBoardX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MIWMd8GF1tL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer = SummaryWriter(summary_filename)\n",
        "\n",
        "manager.export_agent_params(summary_filename + \"/\"+\"agent_params.json\")\n",
        "manager.export_environment_params(summary_filename + \"/\"+\"environment_params.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VnM12lYF6Tk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#Contador global de ejecuciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVbbs8v7F8JR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step_num = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KshEAhVWF_YJ",
        "colab_type": "text"
      },
      "source": [
        "# Habilitar entrenamiento por gráfica o CPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEbloYS7F-tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = manager.get_agent_params()['use_cuda']\n",
        "device = torch.device(\"cuda:\"+str(args.gpu_id) if torch.cuda.is_available() and use_cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyhWLSs4GEBL",
        "colab_type": "text"
      },
      "source": [
        "# Habilitar la semilla aleatoria para poder reproducir el experimento a posteriori"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPG2SQKIEhs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = manager.get_agent_params()['seed']\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "if torch.cuda.is_available() and use_cuda:\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnBymhWkEau6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepQLearner(object):\n",
        "    def __init__(self, obs_shape, action_shape, params):\n",
        "       \n",
        "        self.params = params\n",
        "        self.gamma = self.params['gamma']\n",
        "        self.learning_rate = self.params['learning_rate']\n",
        "        self.best_mean_reward = -float(\"inf\")\n",
        "        self.best_reward = -float(\"inf\")\n",
        "        self.training_steps_completed = 0\n",
        "        self.action_shape = action_shape\n",
        "        \n",
        "        if len(obs_shape)  == 1: ## Solo tenemos una dimensión del espacio de observaciones\n",
        "            self.DQN = SLP\n",
        "        elif len(obs_shape) == 3: ## El estado de observaciones es una imagen/3D\n",
        "            self.DQN = CNN\n",
        "            \n",
        "        self.Q = self.DQN(obs_shape, action_shape, device).to(device)\n",
        "        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr = self.learning_rate)\n",
        "        \n",
        "        if self.params['use_target_network']:\n",
        "            self.Q_target = self.DQN(obs_shape, action_shape, device).to(device)\n",
        "           \n",
        "        self.policy = self.epsilon_greedy_Q\n",
        "        self.epsilon_max = self.params['epsilon_max']\n",
        "        self.epsilon_min = self.params['epsilon_min']\n",
        "        self.epsilon_decay = LinearDecaySchedule(initial_value = self.epsilon_max,\n",
        "                                                 final_value = self.epsilon_min, \n",
        "                                                 max_steps = self.params['epsilon_decay_final_step'])\n",
        "        self.step_num = 0\n",
        "        \n",
        "        self.memory = ExperienceMemory(capacity = int(self.params['experience_memory_size']))\n",
        "        \n",
        "         \n",
        "    def get_action(self, obs):\n",
        "        obs = np.array(obs)\n",
        "        obs = obs / 255.0\n",
        "        if len(obs.shape) == 3: # tenemos una imagen\n",
        "            if obs.shape[2] < obs.shape[0]: # WxHxC -> C x H x W\n",
        "                obs = obs.reshape(obs.shape[2], obs.shape[1], obs.shape[0])\n",
        "            obs = np.expand_dims(obs, 0)   \n",
        "        return self.policy(obs)\n",
        "    \n",
        "    def epsilon_greedy_Q(self, obs):\n",
        "        writer.add_scalar(\"DQL/epsilon\", self.epsilon_decay(self.step_num), self.step_num)\n",
        "        self.step_num +=1\n",
        "        if random.random() < self.epsilon_decay(self.step_num) and not self.params[\"test\"]:\n",
        "            action = random.choice([a for a in range(self.action_shape)])\n",
        "        else:\n",
        "            action = np.argmax(self.Q(obs).data.to(torch.device('cpu')).numpy())   \n",
        "        return action\n",
        "        \n",
        "        \n",
        "    def learn(self, obs, action, reward, next_obs, done):\n",
        "        if done:\n",
        "            td_target = reward + 0.0\n",
        "        else: \n",
        "            td_target = reward + self.gamma * torch.max(self.Q(next_obs))\n",
        "        td_error = torch.nn.functional.mse_loss(self.Q(obs)[action], td_target)\n",
        "        self.Q_optimizer.zero_grad()\n",
        "        td_error.backward()\n",
        "        writer.add_scalar(\"DQL/td_error\", td_error.mean(), self.step_num)\n",
        "        self.Q_optimizer.step()\n",
        "        \n",
        "    def replay_experience(self, batch_size = None):\n",
        "        \"\"\"\n",
        "        Vuelve a jugar usando la experiencia aleatoria almacenada\n",
        "        :param batch_size: Tamaño de la muestra a tomar de la memoria\n",
        "        :return: \n",
        "        \"\"\"\n",
        "        batch_size = batch_size if batch_size is not None else self.params['replay_batch_size']\n",
        "        experience_batch = self.memory.sample(batch_size)\n",
        "        self.learn_from_batch_experience(experience_batch)   \n",
        "        self.training_steps_completed += 1\n",
        "      \n",
        "    def learn_from_batch_experience(self, experiences):\n",
        "        \"\"\"\n",
        "        Actualiza la red neuronal profunda en base a lo aprendido en el conjunto de experiencias anteriores\n",
        "        :param experiences: fragmento de recuerdos anteriores\n",
        "        :return: \n",
        "        \"\"\"\n",
        "        batch_xp = Experience(*zip(*experiences))\n",
        "        obs_batch = np.array(batch_xp.obs)/255.0\n",
        "        action_batch = np.array(batch_xp.action)\n",
        "        reward_batch = np.array(batch_xp.reward)\n",
        "        \n",
        "        if self.params[\"clip_reward\"]:\n",
        "            reward_batch = np.sign(reward_batch)\n",
        "        next_obs_batch = np.array(batch_xp.next_obs)/255.0\n",
        "        done_batch = np.array(batch_xp.done)\n",
        "        \n",
        "        \n",
        "        if self.params['use_target_network']:\n",
        "            if self.step_num % self.params['target_network_update_frequency'] == 0:\n",
        "                self.Q_target.load_state_dict(self.Q.state_dict())\n",
        "            td_target = reward_batch + ~done_batch *\\\n",
        "                        np.tile(self.gamma, len(next_obs_batch)) * \\\n",
        "                        torch.max(self.Q_target(next_obs_batch),1)[0].data.tolist()\n",
        "            td_target = torch.from_numpy(td_target)\n",
        "\n",
        "        else: \n",
        "            td_target = reward_batch + ~done_batch * \\\n",
        "                        np.tile(self.gamma, len(next_obs_batch)) * \\\n",
        "                        torch.max(self.Q(next_obs_batch).detach(),1)[0].data.tolist()\n",
        "            td_target = torch.from_numpy(td_target)\n",
        "\n",
        "        \n",
        "        td_target = td_target.to(device)\n",
        "        action_idx = torch.from_numpy(action_batch).to(device)\n",
        "        td_error = torch.nn.functional.mse_loss(\n",
        "                self.Q(obs_batch).gather(1, action_idx.view(-1,1)),\n",
        "                td_target.float().unsqueeze(1))\n",
        "        \n",
        "        self.Q_optimizer.zero_grad()\n",
        "        td_error.mean().backward()\n",
        "        self.Q_optimizer.step()\n",
        "        \n",
        "    def save(self, env_name):\n",
        "        model_save_name = 'model.pt'\n",
        "        path = F\"/content/drive/My Drive/{model_save_name}\" \n",
        "        file_name = self.params['save_dir']+\"DQL_\"+env_name+\".ptm\"\n",
        "        agent_state = {\"Q\": self.Q.state_dict(),\n",
        "                       \"best_mean_reward\": self.best_mean_reward,\n",
        "                       \"best_reward\": self.best_reward}\n",
        "        torch.save(agent_state, file_name)\n",
        "        print(\"Estado del agente guardado en : \", file_name)\n",
        "        \n",
        "        \n",
        "    def load(self, env_name):\n",
        "        path = F\"/content/drive/My Drive/trained_models/model.pt\"\n",
        "        file_name = self.params['load_dir']+\"DQL_\"+env_name+\".ptm\"\n",
        "        agent_state = torch.load(file_name, map_location = lambda storage, loc: storage)\n",
        "        self.Q.load_state_dict(agent_state[\"Q\"])\n",
        "        self.Q.to(device)\n",
        "        self.best_mean_reward = agent_state[\"best_mean_reward\"]\n",
        "        self.best_reward = agent_state[\"best_reward\"]\n",
        "        print(\"Cargado del modelo Q desde\", file_name,\n",
        "              \"que hasta el momento tiene una mejor recompensa media de: \",self.best_mean_reward,\n",
        "              \" y una recompensa máxima de: \", self.best_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg5F6OERzRvD",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d39qUlVjENDH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "938cc95a-a158-45b5-acea-7a1057716fe6"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env_conf = manager.get_environment_params()\n",
        "    env_conf[\"env_name\"] = args.env\n",
        "    \n",
        "    if args.test:\n",
        "        env_conf[\"episodic_life\"] = False\n",
        "    reward_type = \"LIFE\" if env_conf[\"episodic_life\"] else \"GAME\"\n",
        "    \n",
        "    custom_region_available = False\n",
        "    for key, value in env_conf[\"useful_region\"].items():\n",
        "        if key in args.env:\n",
        "            env_conf[\"useful_region\"] = value\n",
        "            custom_region_available = True\n",
        "            break\n",
        "    if custom_region_available is not True:\n",
        "        env_conf[\"useful_region\"] = env_conf[\"useful_region\"][\"Default\"]\n",
        "    print(\"Configuración a utilizar:\", env_conf)\n",
        "    \n",
        "    atari_env = False\n",
        "    for game in Atari.get_games_list():\n",
        "        if game.replace(\"_\", \"\") in args.env.lower():\n",
        "            atari_env = True\n",
        "    \n",
        "    if atari_env:\n",
        "        environment = Atari.make_env(args.env, env_conf)\n",
        "    else:\n",
        "        environment = env_utils.ResizeReshapeFrames(gym.make(args.env))\n",
        "        \n",
        "    obs_shape = environment.observation_space.shape\n",
        "    action_shape = environment.action_space.n\n",
        "    agent_params = manager.get_agent_params()\n",
        "    agent_params[\"test\"] = args.test\n",
        "    agent_params[\"clip_reward\"] = env_conf[\"clip_reward\"]\n",
        "    agent = DeepQLearner(obs_shape, action_shape, agent_params)\n",
        "    \n",
        "    episode_rewards = list()\n",
        "    previous_checkpoint_mean_ep_rew = agent.best_mean_reward\n",
        "    num_improved_episodes_before_checkpoint = 0\n",
        "    if agent_params['load_trained_model']:\n",
        "        try:\n",
        "            agent.load(env_conf['env_name'])\n",
        "            previous_checkpoint_mean_ep_rew = agent.best_mean_reward\n",
        "        except FileNotFoundError:\n",
        "            print(\"ERROR: no existe ningún modelo entrenado para este entorno. Empezamos desde cero\")\n",
        "\n",
        "    \n",
        "    episode = 0\n",
        "    while global_step_num < agent_params['max_training_steps']:\n",
        "        obs = environment.reset()\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        step = 0\n",
        "        while not done: \n",
        "            if env_conf['render'] or args.render:\n",
        "                environment.render()\n",
        "            \n",
        "            action = agent.get_action(obs)\n",
        "            next_obs, reward, done, info = environment.step(action)\n",
        "            agent.memory.store(Experience(obs, action, reward, next_obs, done))\n",
        "            \n",
        "            obs = next_obs\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "            global_step_num += 1\n",
        "            \n",
        "            if done is True:\n",
        "                episode += 1\n",
        "                episode_rewards.append(total_reward)\n",
        "            \n",
        "                if total_reward > agent.best_reward:\n",
        "                    agent.best_reward = total_reward\n",
        "                \n",
        "                if np.mean(episode_rewards) > previous_checkpoint_mean_ep_rew: \n",
        "                    num_improved_episodes_before_checkpoint += 1\n",
        "                \n",
        "                if num_improved_episodes_before_checkpoint >= agent_params['save_freq']:\n",
        "                    previous_checkpoint_mean_ep_rew = np.mean(episode_rewards)\n",
        "                    agent.best_mean_reward = np.mean(episode_rewards)\n",
        "                    agent.save(env_conf['env_name'])\n",
        "                    num_improved_episodes_before_checkpoint = 0\n",
        "                \n",
        "                print(\"\\n Episodio #{} finalizado con {} iteraciones. Con {} estados: recompensa = {}, recompensa media = {:.2f}, mejor recompensa = {}\".\n",
        "                      format(episode, step+1, reward_type, total_reward, np.mean(episode_rewards), agent.best_reward))\n",
        "                \n",
        "                writer.add_scalar(\"main/ep_reward\", total_reward, global_step_num)\n",
        "                writer.add_scalar(\"main/mean_ep_reward\", np.mean(episode_rewards), global_step_num)\n",
        "                writer.add_scalar(\"main/max_ep_reward\", agent.best_reward, global_step_num)\n",
        "                \n",
        "                if agent.memory.get_size() >= 2*agent_params['replay_start_size'] and not args.test:\n",
        "                    agent.replay_experience()\n",
        "                    \n",
        "                break\n",
        "            \n",
        "    environment.close()\n",
        "    writer.close()\n",
        "    show_videos()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Configuración a utilizar: {'type': 'Atari', 'episodic_life': True, 'clip_reward': True, 'skip_rate': 4, 'num_frames_to_stack': 4, 'render': False, 'normalize_observation': False, 'useful_region': {'crop1': 30, 'crop2': 30, 'dimension2': 80}, 'env_name': 'SeaquestNoFrameskip-v4'}\n",
            "ERROR: no existe ningún modelo entrenado para este entorno. Empezamos desde cero\n",
            "\n",
            " Episodio #1 finalizado con 83 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.00, mejor recompensa = 0.0\n",
            "\n",
            " Episodio #2 finalizado con 95 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.00, mejor recompensa = 0.0\n",
            "\n",
            " Episodio #3 finalizado con 77 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.00, mejor recompensa = 0.0\n",
            "\n",
            " Episodio #4 finalizado con 324 iteraciones. Con LIFE estados: recompensa = 7.0, recompensa media = 1.75, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #5 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.40, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #6 finalizado con 116 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.33, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #7 finalizado con 163 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.29, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #8 finalizado con 113 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.25, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #9 finalizado con 137 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.22, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #10 finalizado con 68 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #11 finalizado con 60 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #12 finalizado con 64 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.92, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #13 finalizado con 178 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.92, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #14 finalizado con 106 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.86, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #15 finalizado con 131 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.87, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #16 finalizado con 144 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.88, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #17 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.82, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #18 finalizado con 93 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.78, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #19 finalizado con 134 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.84, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #20 finalizado con 229 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.95, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #21 finalizado con 135 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.95, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #22 finalizado con 77 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.91, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #23 finalizado con 131 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.91, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #24 finalizado con 168 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.96, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #25 finalizado con 106 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.92, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #26 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.88, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #27 finalizado con 160 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.93, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #28 finalizado con 163 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.93, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #29 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.90, mejor recompensa = 7.0\n",
            "\n",
            " Episodio #30 finalizado con 403 iteraciones. Con LIFE estados: recompensa = 8.0, recompensa media = 1.13, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #31 finalizado con 171 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #32 finalizado con 158 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #33 finalizado con 81 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #34 finalizado con 126 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.03, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #35 finalizado con 116 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.06, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #36 finalizado con 121 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.08, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #37 finalizado con 128 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.11, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #38 finalizado con 101 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #39 finalizado con 91 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #40 finalizado con 81 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #41 finalizado con 246 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.10, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #42 finalizado con 66 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #43 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #44 finalizado con 95 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #45 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #46 finalizado con 65 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #47 finalizado con 125 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.02, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #48 finalizado con 65 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #49 finalizado con 81 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 8.0\n",
            "Estado del agente guardado en :  /content/drive/My Drive/DQL_SeaquestNoFrameskip-v4.ptm\n",
            "\n",
            " Episodio #50 finalizado con 99 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.96, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #51 finalizado con 111 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.94, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #52 finalizado con 98 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.92, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #53 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.91, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #54 finalizado con 115 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.89, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #55 finalizado con 109 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.87, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #56 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.86, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #57 finalizado con 100 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.84, mejor recompensa = 8.0\n",
            "\n",
            " Episodio #58 finalizado con 584 iteraciones. Con LIFE estados: recompensa = 10.0, recompensa media = 1.00, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #59 finalizado con 247 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.03, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #60 finalizado con 90 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #61 finalizado con 86 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #62 finalizado con 97 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #63 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.97, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #64 finalizado con 116 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.95, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #65 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.94, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #66 finalizado con 69 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.92, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #67 finalizado con 138 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.94, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #68 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.93, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #69 finalizado con 82 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.91, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #70 finalizado con 86 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.90, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #71 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #72 finalizado con 115 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #73 finalizado con 141 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #74 finalizado con 123 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.91, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #75 finalizado con 148 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.91, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #76 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #77 finalizado con 74 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.88, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #78 finalizado con 171 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.88, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #79 finalizado con 103 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #80 finalizado con 219 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.90, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #81 finalizado con 87 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #82 finalizado con 62 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.88, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #83 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #84 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.86, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #85 finalizado con 81 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.85, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #86 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.84, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #87 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.83, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #88 finalizado con 129 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.83, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #89 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.82, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #90 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.81, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #91 finalizado con 60 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.80, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #92 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.79, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #93 finalizado con 82 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.78, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #94 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.78, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #95 finalizado con 149 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.79, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #96 finalizado con 305 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.81, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #97 finalizado con 145 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.82, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #98 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.82, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #99 finalizado con 92 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.81, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #100 finalizado con 92 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.80, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #101 finalizado con 111 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.79, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #102 finalizado con 139 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.80, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #103 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.80, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #104 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.79, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #105 finalizado con 323 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 0.82, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #106 finalizado con 89 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.81, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #107 finalizado con 124 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.82, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #108 finalizado con 124 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.82, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #109 finalizado con 102 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.82, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #110 finalizado con 122 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.82, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #111 finalizado con 93 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.81, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #112 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.80, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #113 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.80, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #114 finalizado con 122 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.80, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #115 finalizado con 114 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.80, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #116 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.79, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #117 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.79, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #118 finalizado con 59 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.78, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #119 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.77, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #120 finalizado con 179 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.79, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #121 finalizado con 62 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.79, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #122 finalizado con 306 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.80, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #123 finalizado con 175 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.82, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #124 finalizado con 77 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.81, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #125 finalizado con 158 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.82, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #126 finalizado con 200 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.84, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #127 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.83, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #128 finalizado con 131 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.84, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #129 finalizado con 159 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.85, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #130 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.85, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #131 finalizado con 160 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.85, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #132 finalizado con 253 iteraciones. Con LIFE estados: recompensa = 5.0, recompensa media = 0.88, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #133 finalizado con 65 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #134 finalizado con 96 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #135 finalizado con 173 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #136 finalizado con 122 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.88, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #137 finalizado con 103 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #138 finalizado con 110 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.86, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #139 finalizado con 75 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.86, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #140 finalizado con 124 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.86, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #141 finalizado con 143 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #142 finalizado con 131 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #143 finalizado con 138 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.88, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #144 finalizado con 594 iteraciones. Con LIFE estados: recompensa = 7.0, recompensa media = 0.92, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #145 finalizado con 99 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.92, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #146 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.91, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #147 finalizado con 110 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.90, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #148 finalizado con 127 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.91, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #149 finalizado con 58 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.91, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #150 finalizado con 132 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.91, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #151 finalizado con 104 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.90, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #152 finalizado con 274 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.91, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #153 finalizado con 144 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.91, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #154 finalizado con 66 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.90, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #155 finalizado con 203 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.91, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #156 finalizado con 77 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.90, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #157 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.90, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #158 finalizado con 148 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.90, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #159 finalizado con 98 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #160 finalizado con 69 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #161 finalizado con 161 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #162 finalizado con 126 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.88, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #163 finalizado con 153 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #164 finalizado con 113 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #165 finalizado con 117 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #166 finalizado con 134 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #167 finalizado con 285 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 0.91, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #168 finalizado con 101 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.90, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #169 finalizado con 97 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.90, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #170 finalizado con 119 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.90, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #171 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #172 finalizado con 121 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #173 finalizado con 57 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.88, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #174 finalizado con 141 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #175 finalizado con 75 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.88, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #176 finalizado con 80 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.88, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #177 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #178 finalizado con 61 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #179 finalizado con 115 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.86, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #180 finalizado con 129 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.86, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #181 finalizado con 163 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #182 finalizado con 122 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #183 finalizado con 66 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #184 finalizado con 77 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.86, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #185 finalizado con 68 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.86, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #186 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.85, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #187 finalizado con 108 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.85, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #188 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.85, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #189 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.84, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #190 finalizado con 97 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.84, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #191 finalizado con 266 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.85, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #192 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.84, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #193 finalizado con 74 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.84, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #194 finalizado con 160 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.85, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #195 finalizado con 114 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.85, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #196 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.84, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #197 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.84, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #198 finalizado con 338 iteraciones. Con LIFE estados: recompensa = 5.0, recompensa media = 0.86, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #199 finalizado con 324 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 0.88, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #200 finalizado con 107 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.89, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #201 finalizado con 82 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.88, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #202 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.88, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #203 finalizado con 87 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #204 finalizado con 83 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #205 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.86, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #206 finalizado con 231 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #207 finalizado con 129 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #208 finalizado con 116 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.88, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #209 finalizado con 99 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.87, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #210 finalizado con 476 iteraciones. Con LIFE estados: recompensa = 7.0, recompensa media = 0.90, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #211 finalizado con 265 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 0.91, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #212 finalizado con 268 iteraciones. Con LIFE estados: recompensa = 8.0, recompensa media = 0.95, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #213 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.94, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #214 finalizado con 98 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.94, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #215 finalizado con 171 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.94, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #216 finalizado con 157 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.94, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #217 finalizado con 281 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.95, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #218 finalizado con 117 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #219 finalizado con 83 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.95, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #220 finalizado con 365 iteraciones. Con LIFE estados: recompensa = 7.0, recompensa media = 0.98, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #221 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #222 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.97, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #223 finalizado con 103 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.97, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #224 finalizado con 99 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.97, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #225 finalizado con 87 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #226 finalizado con 128 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #227 finalizado con 127 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #228 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #229 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #230 finalizado con 83 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.95, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #231 finalizado con 132 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #232 finalizado con 99 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.95, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #233 finalizado con 102 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.95, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #234 finalizado con 164 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #235 finalizado con 302 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 0.97, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #236 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.97, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #237 finalizado con 72 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #238 finalizado con 145 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.97, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #239 finalizado con 155 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.97, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #240 finalizado con 109 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.97, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #241 finalizado con 184 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 0.98, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #242 finalizado con 99 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #243 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #244 finalizado con 93 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.97, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #245 finalizado con 95 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.97, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #246 finalizado con 127 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.97, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #247 finalizado con 82 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #248 finalizado con 114 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #249 finalizado con 103 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #250 finalizado con 64 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #251 finalizado con 130 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #252 finalizado con 74 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #253 finalizado con 142 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #254 finalizado con 126 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.96, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #255 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.95, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #256 finalizado con 112 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.95, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #257 finalizado con 116 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.95, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #258 finalizado con 89 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.95, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #259 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.95, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #260 finalizado con 93 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.94, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #261 finalizado con 133 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.94, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #262 finalizado con 68 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.94, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #263 finalizado con 65 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.94, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #264 finalizado con 140 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.94, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #265 finalizado con 107 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.94, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #266 finalizado con 165 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.94, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #267 finalizado con 154 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.94, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #268 finalizado con 366 iteraciones. Con LIFE estados: recompensa = 7.0, recompensa media = 0.97, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #269 finalizado con 106 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.97, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #270 finalizado con 383 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 0.99, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #271 finalizado con 441 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 1.00, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #272 finalizado con 60 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #273 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #274 finalizado con 90 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #275 finalizado con 140 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.00, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #276 finalizado con 277 iteraciones. Con LIFE estados: recompensa = 5.0, recompensa media = 1.01, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #277 finalizado con 115 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.02, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #278 finalizado con 100 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.02, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #279 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #280 finalizado con 69 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #281 finalizado con 120 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #282 finalizado con 189 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.01, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #283 finalizado con 165 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.02, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #284 finalizado con 66 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 10.0\n",
            "Estado del agente guardado en :  /content/drive/My Drive/DQL_SeaquestNoFrameskip-v4.ptm\n",
            "\n",
            " Episodio #285 finalizado con 140 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #286 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #287 finalizado con 121 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.01, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #288 finalizado con 95 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #289 finalizado con 128 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.01, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #290 finalizado con 131 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.02, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #291 finalizado con 103 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #292 finalizado con 99 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #293 finalizado con 491 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 1.03, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #294 finalizado con 171 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.03, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #295 finalizado con 106 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #296 finalizado con 218 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.04, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #297 finalizado con 61 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #298 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #299 finalizado con 104 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #300 finalizado con 74 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #301 finalizado con 95 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #302 finalizado con 135 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.02, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #303 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #304 finalizado con 180 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.03, mejor recompensa = 10.0\n",
            "\n",
            " Episodio #305 finalizado con 553 iteraciones. Con LIFE estados: recompensa = 11.0, recompensa media = 1.06, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #306 finalizado con 129 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.06, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #307 finalizado con 112 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.06, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #308 finalizado con 199 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.06, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #309 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #310 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #311 finalizado con 110 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #312 finalizado con 149 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.06, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #313 finalizado con 72 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #314 finalizado con 65 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #315 finalizado con 140 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.05, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #316 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #317 finalizado con 127 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.05, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #318 finalizado con 69 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #319 finalizado con 265 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.06, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #320 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #321 finalizado con 209 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.06, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #322 finalizado con 167 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.07, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #323 finalizado con 120 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.07, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #324 finalizado con 150 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.07, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #325 finalizado con 106 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.07, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #326 finalizado con 112 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.07, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #327 finalizado con 85 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #328 finalizado con 81 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #329 finalizado con 82 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #330 finalizado con 120 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #331 finalizado con 82 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #332 finalizado con 68 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #333 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #334 finalizado con 102 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #335 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #336 finalizado con 134 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.05, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #337 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #338 finalizado con 61 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #339 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #340 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "Estado del agente guardado en :  /content/drive/My Drive/DQL_SeaquestNoFrameskip-v4.ptm\n",
            "\n",
            " Episodio #341 finalizado con 74 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #342 finalizado con 75 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #343 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #344 finalizado con 170 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #345 finalizado con 59 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #346 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #347 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #348 finalizado con 114 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #349 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #350 finalizado con 208 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #351 finalizado con 87 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #352 finalizado con 69 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #353 finalizado con 86 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #354 finalizado con 89 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #355 finalizado con 97 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #356 finalizado con 92 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #357 finalizado con 126 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #358 finalizado con 225 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #359 finalizado con 64 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #360 finalizado con 85 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #361 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #362 finalizado con 60 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #363 finalizado con 118 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #364 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #365 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #366 finalizado con 75 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #367 finalizado con 174 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #368 finalizado con 196 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #369 finalizado con 68 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #370 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #371 finalizado con 87 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #372 finalizado con 77 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #373 finalizado con 311 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #374 finalizado con 121 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #375 finalizado con 91 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #376 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #377 finalizado con 144 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #378 finalizado con 75 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #379 finalizado con 87 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #380 finalizado con 131 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #381 finalizado con 74 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #382 finalizado con 136 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #383 finalizado con 104 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #384 finalizado con 161 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #385 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.97, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #386 finalizado con 265 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #387 finalizado con 217 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #388 finalizado con 64 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #389 finalizado con 117 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #390 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #391 finalizado con 120 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #392 finalizado con 167 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #393 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #394 finalizado con 130 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #395 finalizado con 61 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #396 finalizado con 129 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #397 finalizado con 202 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #398 finalizado con 113 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #399 finalizado con 72 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #400 finalizado con 164 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #401 finalizado con 312 iteraciones. Con LIFE estados: recompensa = 5.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #402 finalizado con 159 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #403 finalizado con 98 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #404 finalizado con 82 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #405 finalizado con 86 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #406 finalizado con 65 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #407 finalizado con 137 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #408 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #409 finalizado con 60 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #410 finalizado con 90 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #411 finalizado con 128 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #412 finalizado con 169 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #413 finalizado con 132 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #414 finalizado con 109 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #415 finalizado con 91 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #416 finalizado con 95 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #417 finalizado con 297 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #418 finalizado con 147 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #419 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #420 finalizado con 157 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #421 finalizado con 310 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #422 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #423 finalizado con 85 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #424 finalizado con 140 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #425 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #426 finalizado con 69 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #427 finalizado con 192 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #428 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #429 finalizado con 77 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #430 finalizado con 129 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #431 finalizado con 83 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #432 finalizado con 89 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #433 finalizado con 119 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #434 finalizado con 134 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #435 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #436 finalizado con 358 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #437 finalizado con 185 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #438 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #439 finalizado con 102 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #440 finalizado con 111 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #441 finalizado con 87 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #442 finalizado con 252 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #443 finalizado con 83 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #444 finalizado con 240 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #445 finalizado con 199 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #446 finalizado con 130 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #447 finalizado con 567 iteraciones. Con LIFE estados: recompensa = 5.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #448 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #449 finalizado con 72 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #450 finalizado con 131 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #451 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #452 finalizado con 105 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #453 finalizado con 104 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #454 finalizado con 86 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #455 finalizado con 256 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #456 finalizado con 159 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #457 finalizado con 333 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #458 finalizado con 110 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #459 finalizado con 108 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #460 finalizado con 120 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #461 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #462 finalizado con 130 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #463 finalizado con 114 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #464 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #465 finalizado con 123 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #466 finalizado con 102 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #467 finalizado con 66 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #468 finalizado con 81 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #469 finalizado con 112 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #470 finalizado con 100 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #471 finalizado con 135 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #472 finalizado con 294 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #473 finalizado con 109 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #474 finalizado con 341 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #475 finalizado con 157 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #476 finalizado con 104 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #477 finalizado con 116 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #478 finalizado con 90 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #479 finalizado con 177 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #480 finalizado con 59 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #481 finalizado con 59 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #482 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #483 finalizado con 99 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #484 finalizado con 235 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #485 finalizado con 273 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #486 finalizado con 118 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #487 finalizado con 145 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #488 finalizado con 136 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #489 finalizado con 146 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #490 finalizado con 133 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #491 finalizado con 133 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #492 finalizado con 119 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #493 finalizado con 128 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #494 finalizado con 133 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #495 finalizado con 215 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #496 finalizado con 151 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #497 finalizado con 103 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #498 finalizado con 115 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #499 finalizado con 144 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #500 finalizado con 81 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #501 finalizado con 127 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #502 finalizado con 69 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #503 finalizado con 92 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #504 finalizado con 100 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #505 finalizado con 102 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #506 finalizado con 108 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #507 finalizado con 81 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #508 finalizado con 130 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #509 finalizado con 66 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #510 finalizado con 164 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.04, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #511 finalizado con 118 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #512 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #513 finalizado con 80 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #514 finalizado con 165 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #515 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #516 finalizado con 97 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #517 finalizado con 56 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #518 finalizado con 89 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #519 finalizado con 66 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #520 finalizado con 80 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #521 finalizado con 95 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #522 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #523 finalizado con 68 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #524 finalizado con 112 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #525 finalizado con 104 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #526 finalizado con 115 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #527 finalizado con 77 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #528 finalizado con 160 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #529 finalizado con 74 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #530 finalizado con 125 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #531 finalizado con 125 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #532 finalizado con 116 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #533 finalizado con 105 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #534 finalizado con 80 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #535 finalizado con 75 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #536 finalizado con 120 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #537 finalizado con 210 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #538 finalizado con 113 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #539 finalizado con 214 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #540 finalizado con 98 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #541 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #542 finalizado con 117 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #543 finalizado con 117 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #544 finalizado con 96 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #545 finalizado con 127 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #546 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #547 finalizado con 183 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #548 finalizado con 115 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #549 finalizado con 105 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #550 finalizado con 102 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #551 finalizado con 119 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #552 finalizado con 170 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #553 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #554 finalizado con 101 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #555 finalizado con 171 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #556 finalizado con 72 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #557 finalizado con 124 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #558 finalizado con 75 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #559 finalizado con 170 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #560 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #561 finalizado con 101 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #562 finalizado con 228 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #563 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #564 finalizado con 124 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #565 finalizado con 89 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #566 finalizado con 142 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #567 finalizado con 107 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #568 finalizado con 313 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #569 finalizado con 122 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #570 finalizado con 81 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #571 finalizado con 86 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #572 finalizado con 60 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #573 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #574 finalizado con 239 iteraciones. Con LIFE estados: recompensa = 5.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #575 finalizado con 133 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #576 finalizado con 162 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #577 finalizado con 95 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #578 finalizado con 132 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #579 finalizado con 75 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #580 finalizado con 97 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #581 finalizado con 285 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #582 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #583 finalizado con 147 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #584 finalizado con 103 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #585 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #586 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #587 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #588 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #589 finalizado con 109 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.99, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #590 finalizado con 212 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #591 finalizado con 252 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #592 finalizado con 273 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #593 finalizado con 115 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #594 finalizado con 133 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #595 finalizado con 118 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.00, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #596 finalizado con 328 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #597 finalizado con 233 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #598 finalizado con 100 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #599 finalizado con 115 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #600 finalizado con 92 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #601 finalizado con 98 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #602 finalizado con 172 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #603 finalizado con 108 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #604 finalizado con 215 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #605 finalizado con 99 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #606 finalizado con 290 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #607 finalizado con 75 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #608 finalizado con 137 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #609 finalizado con 103 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #610 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #611 finalizado con 159 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #612 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #613 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #614 finalizado con 158 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #615 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #616 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 11.0\n",
            "\n",
            " Episodio #617 finalizado con 246 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.02, mejor recompensa = 11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaUT9mszEgT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}