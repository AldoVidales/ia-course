{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1SHs6riAj1s",
        "colab_type": "text"
      },
      "source": [
        "# Clonamos el repositorio para obtener los dataSet\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WFfFZvPApeD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e817e779-9fda-407a-ea86-94e84305eefa"
      },
      "source": [
        "!git clone https://github.com/joanby/ia-course.git"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ia-course' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZZLJUFyA6hs",
        "colab_type": "text"
      },
      "source": [
        "# Damos acceso a nuestro Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMVaYKVrA92v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8CYKTxkJB53",
        "colab_type": "text"
      },
      "source": [
        "# Test it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COqkUYzLJBEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls '/content/drive/My Drive' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eas_Lb17J4jY",
        "colab_type": "text"
      },
      "source": [
        "#Google colab tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Kmgf5ZJ5QV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "import glob # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "from google.colab import drive # Montar tu Google drive"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "# Instalar dependencias de Renderizado, tarda alrededor de 45 segundos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDPDMlDo4RqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install python-opengl -y > /dev/null 2>&1\n",
        "!apt install xvfb -y --fix-missing > /dev/null 2>&1\n",
        "!apt-get install ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!apt-get install pyglet > /dev/null 2>&1\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xxsEvp14U-Q",
        "colab_type": "text"
      },
      "source": [
        "# Instalar OpenAi Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCuDUcBL4SCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install piglet > /dev/null 2>&1\n",
        "!pip install 'gym[box2d]' > /dev/null 2>&1\n",
        "#por si quieres algun environment en concreto\n",
        "#!pip install atari_py > /dev/null 2>&1\n",
        "#!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkkUdIpJ5lHQ",
        "colab_type": "text"
      },
      "source": [
        "# Todos los imports necesarios en google colab y helpers para poder visualizar OpenAi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbgBYNVD5i6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5DvRsCZ4b-G",
        "colab_type": "text"
      },
      "source": [
        "# Activamos una vista, seria como crear un plot de una grafica en python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDMnJvk-4cec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dcd88831-9b79-4e9d-f81e-f330c8a5c1b7"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900)) #Puedes modificar el high and width de la pantalla\n",
        "display.start()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fbf77708e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x80iaLF94eOh",
        "colab_type": "text"
      },
      "source": [
        "# Este código crea una pantalla virtual para dibujar imágenes del juego. \n",
        "## Si se ejecuta localmente, simplemente ignóralo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0H-whKQ4fvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "if type(os.environ.get('DISPLAY')) is not str or \\\n",
        "        len(os.environ.get('DISPLAY')) == 0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMqg9TAA4qxy",
        "colab_type": "text"
      },
      "source": [
        "# Funciones de utilidad para permitir la grabación de video del ambiente del gimnasio y su visualización\n",
        "## Para habilitar la visualizacion por pantalla , tan solo haz \"**environment = wrap_env(environment)**\", por ejemplo: **environment = wrap_env(gym.make(\"MountainCar-v0\"))**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKFIMV_l4m2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import glob\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pathlib import Path\n",
        "\n",
        "def show_one_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "\n",
        "        content = ipythondisplay.display(HTML(data='''\n",
        "        <video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "        </video>\n",
        "        '''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Couldn't find video\")\n",
        "\n",
        "def show_videos(video_path='video', prefix=''):\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = gym.wrappers.Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpaK1CCiVVBR",
        "colab_type": "text"
      },
      "source": [
        "# Instalar pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udH-oiduVdI6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "2cf74934-6eef-4b12-a46e-3f29d2cf9030"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install numpy"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeCCtgxDBiGi",
        "colab_type": "text"
      },
      "source": [
        "# Nuestro Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBaH0bgAv1KR",
        "colab_type": "text"
      },
      "source": [
        "En google colab, para importar modulos se hace de manera diferente, ya que los notebooks que crea se guardan en una ruta temporal para despues guardarse en drive, por lo que si quieres cargar modulos externos se tiene que hacer:  \n",
        "**import sys**   \n",
        "**sys.path.append('path-carpeta')**  *#esto añade como una caché de rutas*  \n",
        "*from module import function or class*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMipgf50VLnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/ia-course/tema3/libs/')\n",
        "sys.path.append('/content/ia-course/tema3/utils/')\n",
        "sys.path.append('/content/ia-course/tema3/environments/')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from perceptron import SLP\n",
        "from decay_schedule import LinearDecaySchedule\n",
        "import random\n",
        "import gym\n",
        "from experience_memory import ExperienceMemory, Experience\n",
        "\n",
        "MAX_NUM_EPISODES = 100000\n",
        "STEPS_PER_EPISODE = 300"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A22ETkPLVmIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SwallowQLearner(object):\n",
        "    def __init__(self, environment, learning_rate = 0.005, gamma = 0.98):\n",
        "        self.obs_shape = environment.observation_space.shape\n",
        "        \n",
        "        self.action_shape = environment.action_space.n\n",
        "        self.Q = SLP(self.obs_shape, self.action_shape)\n",
        "        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr = learning_rate)\n",
        "        \n",
        "        self.gamma = gamma\n",
        "        \n",
        "        self.epsilon_max = 1.0\n",
        "        self.epsilon_min = 0.05\n",
        "        self.epsilon_decay = LinearDecaySchedule(initial_value = self.epsilon_max,\n",
        "                                                 final_value = self.epsilon_min, \n",
        "                                                 max_steps = 0.5 * MAX_NUM_EPISODES * STEPS_PER_EPISODE)\n",
        "        self.step_num = 0\n",
        "        self.policy = self.epsilon_greedy_Q\n",
        "        \n",
        "        self.memory = ExperienceMemory(capacity = int(1e5))\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        \n",
        "         \n",
        "    def get_action(self, obs):\n",
        "        return self.policy(obs)\n",
        "    \n",
        "    def epsilon_greedy_Q(self, obs):\n",
        "        if random.random() < self.epsilon_decay(self.step_num):\n",
        "            action = random.choice([a for a in range(self.action_shape)])\n",
        "        else:\n",
        "            action = np.argmax(self.Q(obs).data.to(torch.device(self.device)).numpy())   \n",
        "        self.step_num += 1 ##EN EL VIDEO SE NOS OLVIDÓ SUBIR EL STEP EN UNA UNIDAD\n",
        "        return action\n",
        "        \n",
        "        \n",
        "    def learn(self, obs, action, reward, next_obs):\n",
        "        td_target = reward + self.gamma * torch.max(self.Q(next_obs))\n",
        "        td_error = torch.nn.functional.mse_loss(self.Q(obs)[action], td_target)\n",
        "        self.Q_optimizer.zero_grad()\n",
        "        td_error.backward()\n",
        "        self.Q_optimizer.step()\n",
        "        \n",
        "    def replay_experience(self, batch_size):\n",
        "        \"\"\"\n",
        "        Vuelve a jugar usando la experiencia aleatoria almacenada\n",
        "        :param batch_size: Tamaño de la muestra a tomar de la memoria\n",
        "        :return: \n",
        "        \"\"\"\n",
        "        experience_batch = self.memory.sample(batch_size)\n",
        "        self.learn_from_batch_experience(experience_batch)   \n",
        "      \n",
        "    def learn_from_batch_experience(self, experiences):\n",
        "        \"\"\"\n",
        "        Actualiza la red neuronal profunda en base a lo aprendido en el conjunto de experiencias anteriores\n",
        "        :param experiences: fragmento de recuerdos anteriores\n",
        "        :return: \n",
        "        \"\"\"\n",
        "        batch_xp = Experience(*zip(*experiences))\n",
        "        obs_batch = np.array(batch_xp.obs)\n",
        "        action_batch = np.array(batch_xp.action)\n",
        "        reward_batch = np.array(batch_xp.reward)\n",
        "        next_obs_batch = np.array(batch_xp.next_obs)\n",
        "        done_batch = np.array(batch_xp.done)\n",
        "        \n",
        "        td_target = reward_batch + ~done_batch * \\\n",
        "        np.tile(self.gamma, len(next_obs_batch)) * \\\n",
        "        self.Q(next_obs_batch).detach().max(1)[0].data.numpy()\n",
        "        td_target = torch.from_numpy(td_target)\n",
        "        \n",
        "        td_target = td_target.to(self.device)\n",
        "        action_idx = torch.from_numpy(action_batch).to(self.device)\n",
        "        td_error = torch.nn.functional.mse_loss(\n",
        "                self.Q(obs_batch).gather(1,action_idx.view(-1,1).long()),\n",
        "                td_target.float().unsqueeze(1))\n",
        "        \n",
        "        self.Q_optimizer.zero_grad()\n",
        "        td_error.mean().backward()\n",
        "        self.Q_optimizer.step()\n",
        "        "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg5F6OERzRvD",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIjRror8zQS5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "0fce8c65-92f0-441c-ef85-717b83dab118"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    environment = gym.make(\"CartPole-v0\")\n",
        "    agent = SwallowQLearner(environment)\n",
        "    first_episode = True\n",
        "    episode_rewards = list()\n",
        "    for episode in range(MAX_NUM_EPISODES):\n",
        "        obs = environment.reset()\n",
        "        total_reward = 0.0\n",
        "        for step in range(STEPS_PER_EPISODE):\n",
        "            #environment.render()\n",
        "            action = agent.get_action(obs)\n",
        "            next_obs, reward, done, info = environment.step(action)\n",
        "            agent.memory.store(Experience(obs, action, reward, next_obs, done))\n",
        "            agent.learn(obs, action, reward, next_obs)\n",
        "            \n",
        "            obs = next_obs\n",
        "            total_reward += reward\n",
        "            \n",
        "            if done is True:\n",
        "                if first_episode:\n",
        "                    max_reward = total_reward\n",
        "                    first_episode = False\n",
        "                episode_rewards.append(total_reward)\n",
        "                if total_reward > max_reward:\n",
        "                    max_reward = total_reward\n",
        "                print(\"\\nEpisodio#{} finalizado con {} iteraciones. Recompensa = {}, Recompensa media = {}, Mejor recompensa = {}\".\n",
        "                      format(episode, step+1, total_reward, np.mean(episode_rewards), max_reward))\n",
        "                if agent.memory.get_size()>100:\n",
        "                    agent.replay_experience(32)\n",
        "                break\n",
        "    environment.close()\n",
        "    show_videos()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Episodio#0 finalizado con 26 iteraciones. Recompensa = 26.0, Recompensa media = 26.0, Mejor recompensa = 26.0\n",
            "\n",
            "Episodio#1 finalizado con 33 iteraciones. Recompensa = 33.0, Recompensa media = 29.5, Mejor recompensa = 33.0\n",
            "\n",
            "Episodio#2 finalizado con 35 iteraciones. Recompensa = 35.0, Recompensa media = 31.333333333333332, Mejor recompensa = 35.0\n",
            "\n",
            "Episodio#3 finalizado con 13 iteraciones. Recompensa = 13.0, Recompensa media = 26.75, Mejor recompensa = 35.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-aa1d06178d3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                       format(episode, step+1, total_reward, np.mean(episode_rewards), max_reward))\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-3016f7662b5d>\u001b[0m in \u001b[0;36mreplay_experience\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \"\"\"\n\u001b[1;32m     49\u001b[0m         \u001b[0mexperience_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_from_batch_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn_from_batch_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-3016f7662b5d>\u001b[0m in \u001b[0;36mlearn_from_batch_experience\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0maction_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         td_error = torch.nn.functional.mse_loss(\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 td_target.float().unsqueeze(1))\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ]
    }
  ]
}